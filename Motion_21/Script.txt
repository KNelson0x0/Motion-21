So, Motion-21’s goals can be summarized into three main points.
Motion-21 aims to:
- Teach
- Be Truly Interactive
- and Totally User-Friendly


The majority of applications that teach American Sign Language are only available on mobile devices and the majority of them are flashcard and video based systems. 
While these approaches are fine the key thing in sign language is the motion that allows you to make the sign. 
Motion-21 aims to not only provide similar approaches but also to be able to interact with the user by using their camera input to grade them. 
For grading camera input the main approach we are taking is using Machine Learning to identify signs. 
And even though our goal is to have our model work for the majority of users, we understand that our model may not work for every user all the time. 
One of the features in the works is the ability for a user to ‘Calibrate Motion-21 to them’ meaning just use and train a model based off of their input. 
And of course while machine learning can be complicated, the user experience doesn’t have to be.
We want Motion-21 to be simple to use and navigate. 

And that pretty much sums up the projects goals.